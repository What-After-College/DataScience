{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DoQXpM2LbEMB"
   },
   "source": [
    "# Data Cleaning\n",
    "### Why clean data?\n",
    "The data that we receive and use is not perfect. Numerous factors such as data collection from multiple sources, or data corruption while storing or retrieving data, human errors in entering data, data loss while transferring data on some network, etc, can lead to incomplete, inconsistent, and incorrect data. If we use data as received in our analysis, then we will perform incorrect analysis and any conclusion drawn from the data will be wrong. Therefore, data cleaning is a necessary step before doing any analysis on the data.\n",
    "\n",
    "Data cleaning or cleansing is the process of detecting and correcting inconsistent, incorrect, and extraneous data. Data cleaning involves dealing with\n",
    "\n",
    "* Missing data\n",
    "* Duplicated data\n",
    "* Outliers in the data\n",
    "* Extra data that might not be needed\n",
    "* Inconsistent data\n",
    "* Converting data into a standard\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1RUTozVublfB"
   },
   "source": [
    "### Understanding Data types\n",
    "![data type](https://i.ibb.co/g3k7pj7/image.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QxqIM_MbqoBv"
   },
   "source": [
    "### Handling Missing Value\n",
    "\n",
    "https://machinelearningmastery.com/handle-missing-data-python/\n",
    "\n",
    "\n",
    "https://www.youtube.com/playlist?list=PLE-8p-CwnFPuOjFcbnXLFvSQaHFK3ymUW\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QqmZgfhhpPQW"
   },
   "source": [
    "\n",
    "\n",
    "### Handling Missing Values\n",
    "Deleting Rows\n",
    "\n",
    "Replacing With Mean/Median/Mode\n",
    "\n",
    "Assigning An Unique Category\n",
    "\n",
    "Predicting The Missing Values\n",
    "\n",
    "Using Algorithms Which Support Missing Values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9NSCFL2ipEcc"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dOK6E0HHrLI5"
   },
   "source": [
    "### Handling incosistent data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bwOTTEYrsAjh"
   },
   "source": [
    "### Handling Outlier\n",
    "#### What are outliers?\n",
    "\n",
    "Outliers are observations that are significantly distant from other observations. These do not follow the general trend of the data. Outliers can indicate variation or error in the data. Outliers in a single variable/column are called univariate while outliers in multiple variables/columns are called multivariate.\n",
    "\n",
    "#### **Sources of outliers**\n",
    "\n",
    "Outliers can be caused by a variety of reasons. Some common ones are:\n",
    "\n",
    "Errors in entering data.\n",
    "\n",
    "Errors in measuring data, e.g., errors in the measuring instrument.\n",
    "Errors in collecting and merging data from multiple sources.\n",
    "Errors in processing data.\n",
    "Natural variance because of some unknown reason.\n",
    "#### **Types of outliers**\n",
    "\n",
    "Outliers can be classified into three broad categories:\n",
    "\n",
    "1. Point or Global outliers #\n",
    "These are observations that deviate from all of the other observations, e.g., if the temperature is recorded as 100 degrees Celsius, or a person who usually spends $100 in a week spends $500 this week.\n",
    "\n",
    "![alt text](https://i.ibb.co/8jwDx6N/image.png)\n",
    "\n",
    "2. Contextual or Conditional outliers #\n",
    "These are data points that are not outliers globally, but are outliers in their own context. If we look at a subset of the data, then we are looking in a context.\n",
    "For instance, a sudden unusual temperature drop in the summer season is considered a contextual outlier where the context is the summer season.\n",
    "Another example could be if the price of a good is $15 and its price falls below $10 during the Christmas period. If its price falls below $10 in July, then that would be a contextual outlier with the month of July being the context.\n",
    "\n",
    "3. Collective outliers \n",
    "These are a group of observations that are outliers globally from the rest of the observations but are not outliers within the group. An example could be a sudden increase in stock transactions of a particular company during a month or unusual delays in shipping orders over a period of three days.\n",
    "\n",
    "\n",
    "![alt text](https://i.ibb.co/xmpVc7D/image.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9Mk6PhHguXX3"
   },
   "source": [
    "### Detecting Outlier\n",
    "\n",
    "Outlier detection\n",
    "* Box plots and Quantile ranges\n",
    "* Scatter plots\n",
    "* Z - Score\n",
    "\n",
    "https://medium.com/@swethalakshmanan14/outlier-detection-and-treatment-a-beginners-guide-c44af0699754\n",
    "\n",
    "outlier detection playlist\n",
    "\n",
    "https://www.youtube.com/playlist?list=PLeo1K3hjS3ut5olrDIeVXk9N3Q7mKhDxO\n",
    "\n",
    "Github Link\n",
    "\n",
    "https://github.com/codebasics/py/tree/master/ML/FeatureEngineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardisation,Normalisation and Scaling\n",
    "\n",
    "Normalisation\n",
    "\n",
    "Normalization is a technique often applied as part of data preparation for machine learning. The goal of normalization is to change the values of numeric columns in the dataset to use a common scale, without distorting differences in the ranges of values or losing information. Normalization is also required for some algorithms to model the data correctly.\n",
    "\n",
    "For example, assume your input dataset contains one column with values ranging from 0 to 1, and another column with values ranging from 10,000 to 100,000. The great difference in the scale of the numbers could cause problems when you attempt to combine the values as features during modeling.\n",
    "\n",
    "Normalization avoids these problems by creating new values that maintain the general distribution and ratios in the source data, while keeping values within a scale applied across all numeric columns used in the model.\n",
    "\n",
    "\n",
    "Standardization Vs Normalization\n",
    "\n",
    "https://www.youtube.com/watch?v=mnKm3YP56PY\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5tgvCCJCvhcX"
   },
   "source": [
    "\n",
    "# Kaggle Introduction\n",
    "\n",
    "visit Kaggle Notebook show thwm different areas of website\n",
    "\n",
    "## competetion Time\n",
    "\n",
    "## Kaggle Notebook for data analysis of titanic dataset\n",
    "# Now go for kaggle Notebook\n",
    "https://colab.research.google.com/drive/1vUthOFMzTV3uDxzkquIijtDzPtmyHnrc?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ubw0TXzdvqlU"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Data_cleaning.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
